@ARTICLE{Brown2024-bs,
  title         = "Large language monkeys: Scaling inference compute with
                   repeated sampling",
  author        = "Brown, Bradley and Juravsky, Jordan and Ehrlich, Ryan and
                   Clark, Ronald and Le, Quoc V and Ré, Christopher and
                   Mirhoseini, Azalia",
  journal       = "arXiv [cs.LG]",
  abstract      = "Scaling the amount of compute used to train language models
                   has dramatically improved their capabilities. However, when
                   it comes to inference, we often limit the amount of compute
                   to only one attempt per problem. Here, we explore inference
                   compute as another axis for scaling by increasing the number
                   of generated samples. Across multiple tasks and models, we
                   observe that coverage - the fraction of problems solved by
                   any attempt - scales with the number of samples over four
                   orders of magnitude. In domains like coding and formal
                   proofs, where all answers can be automatically verified,
                   these increases in coverage directly translate into improved
                   performance. When we apply repeated sampling to SWE-bench
                   Lite, the fraction of issues solved with
                   DeepSeek-V2-Coder-Instruct increases from 15.9\% with one
                   sample to 56\% with 250 samples, outperforming the
                   single-attempt state-of-the-art of 43\% which uses more
                   capable frontier models. Moreover, using current API pricing,
                   amplifying the cheaper DeepSeek model with five samples is
                   more cost-effective and solves more issues than paying a
                   premium for one sample from GPT-4o or Claude 3.5 Sonnet.
                   Interestingly, the relationship between coverage and the
                   number of samples is often log-linear and can be modelled
                   with an exponentiated power law, suggesting the existence of
                   inference-time scaling laws. Finally, we find that
                   identifying correct samples out of many generations remains
                   an important direction for future research in domains without
                   automatic verifiers. When solving math word problems from
                   GSM8K and MATH, coverage with Llama-3 models grows to over
                   95\% with 10,000 samples. However, common methods to pick
                   correct solutions from a sample collection, such as majority
                   voting or reward models, plateau beyond several hundred
                   samples and fail to fully scale with the sample budget.",
  month         =  "31~" # jul,
  year          =  2024,
  url           = "http://arxiv.org/abs/2407.21787",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  keywords      = "o1"
}

@INPROCEEDINGS{Brown2017-of,
  title     = "Libratus: The superhuman {AI} for no-limit poker",
  author    = "Brown, Noam and Sandholm, Tuomas",
  booktitle = "Proceedings of the Twenty-Sixth International Joint Conference on
               Artificial Intelligence",
  publisher = "International Joint Conferences on Artificial Intelligence
               Organization",
  address   = "California",
  abstract  = "No-limit Texas Hold'em is the most popular variant of poker in
               the world. Heads-up no-limit Texas Hold'em is the main benchmark
               challenge for AI in imperfect-information games. We present
               Libratus, the first - and so far only - AI to defeat top human
               professionals in that game. Libratus's architecture features
               three main modules, each of which has new algorithms:
               pre-computing a solution to an abstraction of the game which
               provides a high-level blueprint for the strategy of the AI, a new
               nested subgame-solving algorithm which repeatedly calculates a
               more detailed strategy as play progresses, and a self-improving
               module which augments the pre-computed blueprint over time.",
  month     =  aug,
  year      =  2017,
  url       = "https://www.onlinecasinoground.nl/wp-content/uploads/2018/10/Libratus-super-human-no-limit-poker-Sandholm-Brown.pdf",
  keywords  = "o1"
}

@ARTICLE{Silver2016-ag,
  title     = "Mastering the game of Go with deep neural networks and tree
               search",
  author    = "Silver, David and Huang, Aja and Maddison, Chris J and Guez,
               Arthur and Sifre, Laurent and van den Driessche, George and
               Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam,
               Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik
               and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and
               Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray
               and Graepel, Thore and Hassabis, Demis",
  journal   = "Nature",
  publisher = "Nature Publishing Group",
  volume    =  529,
  number    =  7587,
  pages     = "484--489",
  abstract  = "The game of Go has long been viewed as the most challenging of
               classic games for artificial intelligence owing to its enormous
               search space and the difficulty of evaluating board positions and
               moves. Here we introduce a new approach to computer Go that uses
               'value networks' to evaluate board positions and 'policy
               networks' to select moves. These deep neural networks are trained
               by a novel combination of supervised learning from human expert
               games, and reinforcement learning from games of self-play.
               Without any lookahead search, the neural networks play Go at the
               level of state-of-the-art Monte Carlo tree search programs that
               simulate thousands of random games of self-play. We also
               introduce a new search algorithm that combines Monte Carlo
               simulation with value and policy networks. Using this search
               algorithm, our program AlphaGo achieved a 99.8\% winning rate
               against other Go programs, and defeated the human European Go
               champion by 5 games to 0. This is the first time that a computer
               program has defeated a human professional player in the
               full-sized game of Go, a feat previously thought to be at least a
               decade away.",
  month     =  "28~" # jan,
  year      =  2016,
  url       = "https://www.nature.com/articles/nature16961",
  keywords  = "o1",
  language  = "en"
}

@MISC{Paul-G-Allen-School2024-da,
  title     = "Parables on the Power of Planning in {AI}: From Poker to
               Diplomacy: Noam Brown ({OpenAI})",
  author    = "{Paul G. Allen School}",
  publisher = "Youtube",
  abstract  = "Title: Parables on the Power of Planning in AI: From Poker to
               DiplomacySpeaker: Noam Brown (OpenAI)Date: Thursday, May 23,
               2024Abstract: from Deep Blue in 19...",
  month     =  "17~" # sep,
  year      =  2024,
  url       = "https://www.youtube.com/watch?v=eaAonE58sLU",
  keywords  = "Paul G. Allen School of Computer Science \& Engineering;
               University of Washington;o1"
}

@ARTICLE{Jones2021-di,
  title         = "Scaling scaling laws with board games",
  author        = "Jones, Andy L",
  journal       = "arXiv [cs.LG]",
  abstract      = "The largest experiments in machine learning now require
                   resources far beyond the budget of all but a few
                   institutions. Fortunately, it has recently been shown that
                   the results of these huge experiments can often be
                   extrapolated from the results of a sequence of far smaller,
                   cheaper experiments. In this work, we show that not only can
                   the extrapolation be done based on the size of the model, but
                   on the size of the problem as well. By conducting a sequence
                   of experiments using AlphaZero and Hex, we show that the
                   performance achievable with a fixed amount of compute
                   degrades predictably as the game gets larger and harder.
                   Along with our main result, we further show that the
                   test-time and train-time compute available to an agent can be
                   traded off while maintaining performance.",
  month         =  "7~" # apr,
  year          =  2021,
  url           = "http://arxiv.org/abs/2104.03113",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  keywords      = "o1"
}

@ARTICLE{Welleck2024-yr,
  title         = "From decoding to meta-generation: Inference-time algorithms
                   for large language models",
  author        = "Welleck, Sean and Bertsch, Amanda and Finlayson, Matthew and
                   Schoelkopf, Hailey and Xie, Alex and Neubig, Graham and
                   Kulikov, Ilia and Harchaoui, Zaid",
  journal       = "arXiv [cs.CL]",
  abstract      = "One of the most striking findings in modern research on large
                   language models (LLMs) is that scaling up compute during
                   training leads to better results. However, less attention has
                   been given to the benefits of scaling compute during
                   inference. This survey focuses on these inference-time
                   approaches. We explore three areas under a unified
                   mathematical formalism: token-level generation algorithms,
                   meta-generation algorithms, and efficient generation.
                   Token-level generation algorithms, often called decoding
                   algorithms, operate by sampling a single token at a time or
                   constructing a token-level search space and then selecting an
                   output. These methods typically assume access to a language
                   model's logits, next-token distributions, or probability
                   scores. Meta-generation algorithms work on partial or full
                   sequences, incorporating domain knowledge, enabling
                   backtracking, and integrating external information. Efficient
                   generation methods aim to reduce token costs and improve the
                   speed of generation. Our survey unifies perspectives from
                   three research communities: traditional natural language
                   processing, modern LLMs, and machine learning systems.",
  month         =  "24~" # jun,
  year          =  2024,
  url           = "http://arxiv.org/abs/2406.16838",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  keywords      = "o1"
}

@ARTICLE{Wang2024-xv,
  title         = "Mixture-of-agents enhances large language model capabilities",
  author        = "Wang, Junlin and Wang, Jue and Athiwaratkun, Ben and Zhang,
                   Ce and Zou, James",
  journal       = "arXiv [cs.CL]",
  abstract      = "Recent advances in large language models (LLMs) demonstrate
                   substantial capabilities in natural language understanding
                   and generation tasks. With the growing number of LLMs, how to
                   harness the collective expertise of multiple LLMs is an
                   exciting open direction. Toward this goal, we propose a new
                   approach that leverages the collective strengths of multiple
                   LLMs through a Mixture-of-Agents (MoA) methodology. In our
                   approach, we construct a layered MoA architecture wherein
                   each layer comprises multiple LLM agents. Each agent takes
                   all the outputs from agents in the previous layer as
                   auxiliary information in generating its response. MoA models
                   achieves state-of-art performance on AlpacaEval 2.0, MT-Bench
                   and FLASK, surpassing GPT-4 Omni. For example, our MoA using
                   only open-source LLMs is the leader of AlpacaEval 2.0 by a
                   substantial gap, achieving a score of 65.1\% compared to
                   57.5\% by GPT-4 Omni.",
  month         =  "7~" # jun,
  year          =  2024,
  url           = "http://arxiv.org/abs/2406.04692",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  keywords      = "o1"
}

@INPROCEEDINGS{Yoshida2024-bs,
  title     = "{MAP’s} not dead yet: Uncovering true language model modes by
               conditioning away degeneracy",
  author    = "Yoshida, Davis and Goyal, Kartik and Gimpel, Kevin",
  booktitle = "Proceedings of the 62nd Annual Meeting of the Association for
               Computational Linguistics (Volume 1: Long Papers)",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "16164--16215",
  abstract  = "Davis Yoshida, Kartik Goyal, Kevin Gimpel. Proceedings of the
               62nd Annual Meeting of the Association for Computational
               Linguistics (Volume 1: Long Papers). 2024.",
  year      =  2024,
  url       = "https://aclanthology.org/2024.acl-long.855.pdf",
  keywords  = "o1"
}

@ARTICLE{Wu2024-mt,
  title         = "Inference scaling laws: An empirical analysis of
                   compute-optimal inference for problem-solving with language
                   models",
  author        = "Wu, Yangzhen and Sun, Zhiqing and Li, Shanda and Welleck,
                   Sean and Yang, Yiming",
  journal       = "arXiv [cs.AI]",
  abstract      = "While the scaling laws of large language models (LLMs)
                   training have been extensively studied, optimal inference
                   configurations of LLMs remain underexplored. We study
                   inference scaling laws and compute-optimal inference,
                   focusing on the trade-offs between model sizes and generating
                   additional tokens with different inference strategies. As a
                   first step towards understanding and designing
                   compute-optimal inference methods, we studied
                   cost-performance trade-offs for inference strategies such as
                   greedy search, majority voting, best-of-$n$, weighted voting,
                   and two different tree search algorithms, using different
                   model sizes and compute budgets. Our findings indicate
                   smaller models (e.g., Llemma-7B) can outperform larger models
                   given the same computation budgets, and that smaller models
                   paired with advanced inference algorithms yield
                   Pareto-optimal cost-performance trade-offs. For instance, the
                   Llemma-7B model, equipped with our novel tree search
                   algorithm, consistently outperforms Llemma-34B with standard
                   majority voting on the MATH benchmark across all FLOPs
                   budgets. We hope these findings contribute to a broader
                   understanding of inference scaling laws for LLMs.",
  month         =  "1~" # aug,
  year          =  2024,
  url           = "http://arxiv.org/abs/2408.00724",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  keywords      = "o1"
}

@ARTICLE{Kazemnejad2024-cp,
  title         = "{VinePPO}: Unlocking {RL} potential for {LLM} reasoning
                   through refined credit assignment",
  author        = "Kazemnejad, Amirhossein and Aghajohari, Milad and Portelance,
                   Eva and Sordoni, Alessandro and Reddy, Siva and Courville,
                   Aaron and Roux, Nicolas Le",
  journal       = "arXiv [cs.LG]",
  abstract      = "Large language models (LLMs) are increasingly applied to
                   complex reasoning tasks that require executing several
                   complex steps before receiving any reward. Properly assigning
                   credit to these steps is essential for enhancing model
                   performance. Proximal Policy Optimization (PPO), a
                   state-of-the-art reinforcement learning (RL) algorithm used
                   for LLM finetuning, employs value networks to tackle credit
                   assignment. However, value networks face challenges in
                   predicting the expected cumulative rewards accurately in
                   complex reasoning tasks, often leading to high-variance
                   updates and suboptimal performance. In this work, we
                   systematically evaluate the efficacy of value networks and
                   reveal their significant shortcomings in reasoning-heavy LLM
                   tasks, showing that they barely outperform a random baseline
                   when comparing alternative steps. To address this, we propose
                   VinePPO, a straightforward approach that leverages the
                   flexibility of language environments to compute unbiased
                   Monte Carlo-based estimates, bypassing the need for large
                   value networks. Our method consistently outperforms PPO and
                   other RL-free baselines across MATH and GSM8K datasets with
                   fewer gradient updates (up to 9x), less wall-clock time (up
                   to 3.0x). These results emphasize the importance of accurate
                   credit assignment in RL finetuning of LLM and demonstrate
                   VinePPO's potential as a superior alternative.",
  month         =  "2~" # oct,
  year          =  2024,
  url           = "http://arxiv.org/abs/2410.01679",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  keywords      = "o1"
}

@ARTICLE{Cobbe2021-gt,
  title         = "Training verifiers to solve math word problems",
  author        = "Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and
                   Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert,
                   Matthias and Tworek, Jerry and Hilton, Jacob and Nakano,
                   Reiichiro and Hesse, Christopher and Schulman, John",
  journal       = "arXiv [cs.LG]",
  abstract      = "State-of-the-art language models can match human performance
                   on many tasks, but they still struggle to robustly perform
                   multi-step mathematical reasoning. To diagnose the failures
                   of current models and support research, we introduce GSM8K, a
                   dataset of 8.5K high quality linguistically diverse grade
                   school math word problems. We find that even the largest
                   transformer models fail to achieve high test performance,
                   despite the conceptual simplicity of this problem
                   distribution. To increase performance, we propose training
                   verifiers to judge the correctness of model completions. At
                   test time, we generate many candidate solutions and select
                   the one ranked highest by the verifier. We demonstrate that
                   verification significantly improves performance on GSM8K, and
                   we provide strong empirical evidence that verification scales
                   more effectively with increased data than a finetuning
                   baseline.",
  month         =  "27~" # oct,
  year          =  2021,
  url           = "http://arxiv.org/abs/2110.14168",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  keywords      = "o1"
}

@ARTICLE{Kumar2024-fj,
  title         = "Training language models to self-correct via reinforcement
                   learning",
  author        = "Kumar, Aviral and Zhuang, Vincent and Agarwal, Rishabh and
                   Su, Yi and Co-Reyes, John D and Singh, Avi and Baumli, Kate
                   and Iqbal, Shariq and Bishop, Colton and Roelofs, Rebecca and
                   Zhang, Lei M and McKinney, Kay and Shrivastava, Disha and
                   Paduraru, Cosmin and Tucker, George and Precup, Doina and
                   Behbahani, Feryal and Faust, Aleksandra",
  journal       = "arXiv [cs.LG]",
  abstract      = "Self-correction is a highly desirable capability of large
                   language models (LLMs), yet it has consistently been found to
                   be largely ineffective in modern LLMs. Current methods for
                   training self-correction typically depend on either multiple
                   models, a more advanced model, or additional forms of
                   supervision. To address these shortcomings, we develop a
                   multi-turn online reinforcement learning (RL) approach,
                   SCoRe, that significantly improves an LLM's self-correction
                   ability using entirely self-generated data. To build SCoRe,
                   we first show that variants of supervised fine-tuning (SFT)
                   on offline model-generated correction traces are often
                   insufficient for instilling self-correction behavior. In
                   particular, we observe that training via SFT falls prey to
                   either a distribution mismatch between mistakes made by the
                   data-collection policy and the model's own responses, or to
                   behavior collapse, where learning implicitly prefers only a
                   certain mode of correction behavior that is often not
                   effective at self-correction on test problems. SCoRe
                   addresses these challenges by training under the model's own
                   distribution of self-generated correction traces and using
                   appropriate regularization to steer the learning process into
                   learning a self-correction behavior that is effective at test
                   time as opposed to fitting high-reward responses for a given
                   prompt. This regularization process includes an initial phase
                   of multi-turn RL on a base model to generate a policy
                   initialization that is less susceptible to collapse, followed
                   by using a reward bonus to amplify self-correction. With
                   Gemini 1.0 Pro and 1.5 Flash models, we find that SCoRe
                   achieves state-of-the-art self-correction performance,
                   improving the base models' self-correction by 15.6\% and
                   9.1\% respectively on MATH and HumanEval.",
  month         =  "19~" # sep,
  year          =  2024,
  url           = "http://arxiv.org/abs/2409.12917",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  keywords      = "o1"
}

@ARTICLE{Snell2024-dx,
  title         = "Scaling {LLM} test-time compute optimally can be more
                   effective than scaling model parameters",
  author        = "Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar,
                   Aviral",
  journal       = "arXiv [cs.LG]",
  abstract      = "Enabling LLMs to improve their outputs by using more
                   test-time computation is a critical step towards building
                   generally self-improving agents that can operate on
                   open-ended natural language. In this paper, we study the
                   scaling of inference-time computation in LLMs, with a focus
                   on answering the question: if an LLM is allowed to use a
                   fixed but non-trivial amount of inference-time compute, how
                   much can it improve its performance on a challenging prompt?
                   Answering this question has implications not only on the
                   achievable performance of LLMs, but also on the future of LLM
                   pretraining and how one should tradeoff inference-time and
                   pre-training compute. Despite its importance, little research
                   attempted to understand the scaling behaviors of various
                   test-time inference methods. Moreover, current work largely
                   provides negative results for a number of these strategies.
                   In this work, we analyze two primary mechanisms to scale
                   test-time computation: (1) searching against dense,
                   process-based verifier reward models; and (2) updating the
                   model's distribution over a response adaptively, given the
                   prompt at test time. We find that in both cases, the
                   effectiveness of different approaches to scaling test-time
                   compute critically varies depending on the difficulty of the
                   prompt. This observation motivates applying a
                   ``compute-optimal'' scaling strategy, which acts to most
                   effectively allocate test-time compute adaptively per prompt.
                   Using this compute-optimal strategy, we can improve the
                   efficiency of test-time compute scaling by more than 4x
                   compared to a best-of-N baseline. Additionally, in a
                   FLOPs-matched evaluation, we find that on problems where a
                   smaller base model attains somewhat non-trivial success
                   rates, test-time compute can be used to outperform a 14x
                   larger model.",
  month         =  "6~" # aug,
  year          =  2024,
  url           = "http://arxiv.org/abs/2408.03314",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  keywords      = "o1"
}

@ARTICLE{Zelikman2024-cu,
  title         = "Quiet-{STaR}: Language models can teach themselves to think
                   before speaking",
  author        = "Zelikman, Eric and Harik, Georges and Shao, Yijia and
                   Jayasiri, Varuna and Haber, Nick and Goodman, Noah D",
  journal       = "arXiv [cs.CL]",
  abstract      = "When writing and talking, people sometimes pause to think.
                   Although reasoning-focused works have often framed reasoning
                   as a method of answering questions or completing agentic
                   tasks, reasoning is implicit in almost all written text. For
                   example, this applies to the steps not stated between the
                   lines of a proof or to the theory of mind underlying a
                   conversation. In the Self-Taught Reasoner (STaR, Zelikman et
                   al. 2022), useful thinking is learned by inferring rationales
                   from few-shot examples in question-answering and learning
                   from those that lead to a correct answer. This is a highly
                   constrained setting -- ideally, a language model could
                   instead learn to infer unstated rationales in arbitrary text.
                   We present Quiet-STaR, a generalization of STaR in which LMs
                   learn to generate rationales at each token to explain future
                   text, improving their predictions. We address key challenges,
                   including 1) the computational cost of generating
                   continuations, 2) the fact that the LM does not initially
                   know how to generate or use internal thoughts, and 3) the
                   need to predict beyond individual next tokens. To resolve
                   these, we propose a tokenwise parallel sampling algorithm,
                   using learnable tokens indicating a thought's start and end,
                   and an extended teacher-forcing technique. Encouragingly,
                   generated rationales disproportionately help model
                   difficult-to-predict tokens and improve the LM's ability to
                   directly answer difficult questions. In particular, after
                   continued pretraining of an LM on a corpus of internet text
                   with Quiet-STaR, we find zero-shot improvements on GSM8K
                   (5.9\%$\rightarrow$10.9\%) and CommonsenseQA
                   (36.3\%$\rightarrow$47.2\%) and observe a perplexity
                   improvement of difficult tokens in natural text. Crucially,
                   these improvements require no fine-tuning on these tasks.
                   Quiet-STaR marks a step towards LMs that can learn to reason
                   in a more general and scalable way.",
  month         =  "14~" # mar,
  year          =  2024,
  url           = "http://arxiv.org/abs/2403.09629",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  keywords      = "o1"
}

@ARTICLE{Gandhi2024-vs,
  title         = "Stream of search ({SoS}): Learning to search in language",
  author        = "Gandhi, Kanishk and Lee, Denise and Grand, Gabriel and Liu,
                   Muxin and Cheng, Winson and Sharma, Archit and Goodman, Noah
                   D",
  journal       = "arXiv [cs.LG]",
  abstract      = "Language models are rarely shown fruitful mistakes while
                   training. They then struggle to look beyond the next token,
                   suffering from a snowballing of errors and struggling to
                   predict the consequence of their actions several steps ahead.
                   In this paper, we show how language models can be taught to
                   search by representing the process of search in language, as
                   a flattened string -- a stream of search (SoS). We propose a
                   unified language for search that captures an array of
                   different symbolic search strategies. We demonstrate our
                   approach using the simple yet difficult game of Countdown,
                   where the goal is to combine input numbers with arithmetic
                   operations to reach a target number. We pretrain a
                   transformer-based language model from scratch on a dataset of
                   streams of search generated by heuristic solvers. We find
                   that SoS pretraining increases search accuracy by 25\% over
                   models trained to predict only the optimal search trajectory.
                   We further finetune this model with two policy improvement
                   methods: Advantage-Induced Policy Alignment (APA) and
                   Self-Taught Reasoner (STaR). The finetuned SoS models solve
                   36\% of previously unsolved problems, including problems that
                   cannot be solved by any of the heuristic solvers. Our results
                   indicate that language models can learn to solve problems via
                   search, self-improve to flexibly use different search
                   strategies, and potentially discover new ones.",
  month         =  "1~" # apr,
  year          =  2024,
  url           = "http://arxiv.org/abs/2404.03683",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  keywords      = "o1"
}

@ARTICLE{Wu2024-px,
  title         = "Thinking {LLMs}: General instruction following with thought
                   generation",
  author        = "Wu, Tianhao and Lan, Janice and Yuan, Weizhe and Jiao,
                   Jiantao and Weston, Jason and Sukhbaatar, Sainbayar",
  journal       = "arXiv [cs.CL]",
  abstract      = "LLMs are typically trained to answer user questions or follow
                   instructions similarly to how human experts respond. However,
                   in the standard alignment framework they lack the basic
                   ability of explicit thinking before answering. Thinking is
                   important for complex questions that require reasoning and
                   planning -- but can be applied to any task. We propose a
                   training method for equipping existing LLMs with such
                   thinking abilities for general instruction following without
                   use of additional human data. We achieve this by an iterative
                   search and optimization procedure that explores the space of
                   possible thought generations, allowing the model to learn how
                   to think without direct supervision. For each instruction,
                   the thought candidates are scored using a judge model to
                   evaluate their responses only, and then optimized via
                   preference optimization. We show that this procedure leads to
                   superior performance on AlpacaEval and Arena-Hard, and shows
                   gains from thinking on non-reasoning categories such as
                   marketing, health and general knowledge, in addition to more
                   traditional reasoning \& problem-solving tasks.",
  month         =  "14~" # oct,
  year          =  2024,
  url           = "http://arxiv.org/abs/2410.10630",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  keywords      = "o1"
}

@ARTICLE{Zelikman2022-id,
  title         = "{STaR}: Bootstrapping reasoning with reasoning",
  author        = "Zelikman, Eric and Wu, Yuhuai and Mu, Jesse and Goodman, Noah
                   D",
  journal       = "arXiv [cs.LG]",
  abstract      = "Generating step-by-step ``chain-of-thought'' rationales
                   improves language model performance on complex reasoning
                   tasks like mathematics or commonsense question-answering.
                   However, inducing language model rationale generation
                   currently requires either constructing massive rationale
                   datasets or sacrificing accuracy by using only few-shot
                   inference. We propose a technique to iteratively leverage a
                   small number of rationale examples and a large dataset
                   without rationales, to bootstrap the ability to perform
                   successively more complex reasoning. This technique, the
                   ``Self-Taught Reasoner'' (STaR), relies on a simple loop:
                   generate rationales to answer many questions, prompted with a
                   few rationale examples; if the generated answers are wrong,
                   try again to generate a rationale given the correct answer;
                   fine-tune on all the rationales that ultimately yielded
                   correct answers; repeat. We show that STaR significantly
                   improves performance on multiple datasets compared to a model
                   fine-tuned to directly predict final answers, and performs
                   comparably to fine-tuning a 30$\times$ larger
                   state-of-the-art language model on CommensenseQA. Thus, STaR
                   lets a model improve itself by learning from its own
                   generated reasoning.",
  month         =  "27~" # mar,
  year          =  2022,
  url           = "http://arxiv.org/abs/2203.14465",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  keywords      = "o1"
}

@ARTICLE{Kirchner2024-cu,
  title         = "Prover-Verifier Games improve legibility of {LLM} outputs",
  author        = "Kirchner, Jan Hendrik and Chen, Yining and Edwards, Harri and
                   Leike, Jan and McAleese, Nat and Burda, Yuri",
  journal       = "arXiv [cs.CL]",
  abstract      = "One way to increase confidence in the outputs of Large
                   Language Models (LLMs) is to support them with reasoning that
                   is clear and easy to check -- a property we call legibility.
                   We study legibility in the context of solving grade-school
                   math problems and show that optimizing chain-of-thought
                   solutions only for answer correctness can make them less
                   legible. To mitigate the loss in legibility, we propose a
                   training algorithm inspired by Prover-Verifier Game from Anil
                   et al. (2021). Our algorithm iteratively trains small
                   verifiers to predict solution correctness, ``helpful''
                   provers to produce correct solutions that the verifier
                   accepts, and ``sneaky'' provers to produce incorrect
                   solutions that fool the verifier. We find that the helpful
                   prover's accuracy and the verifier's robustness to
                   adversarial attacks increase over the course of training.
                   Furthermore, we show that legibility training transfers to
                   time-constrained humans tasked with verifying solution
                   correctness. Over course of LLM training human accuracy
                   increases when checking the helpful prover's solutions, and
                   decreases when checking the sneaky prover's solutions. Hence,
                   training for checkability by small verifiers is a plausible
                   technique for increasing output legibility. Our results
                   suggest legibility training against small verifiers as a
                   practical avenue for increasing legibility of large LLMs to
                   humans, and thus could help with alignment of superhuman
                   models.",
  month         =  "18~" # jul,
  year          =  2024,
  url           = "http://arxiv.org/abs/2407.13692",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  keywords      = "o1"
}

@ARTICLE{Uesato2022-aw,
  title         = "Solving math word problems with process- and outcome-based
                   feedback",
  author        = "Uesato, Jonathan and Kushman, Nate and Kumar, Ramana and
                   Song, Francis and Siegel, Noah and Wang, Lisa and Creswell,
                   Antonia and Irving, Geoffrey and Higgins, Irina",
  journal       = "arXiv [cs.LG]",
  abstract      = "Recent work has shown that asking language models to generate
                   reasoning steps improves performance on many reasoning tasks.
                   When moving beyond prompting, this raises the question of how
                   we should supervise such models: outcome-based approaches
                   which supervise the final result, or process-based approaches
                   which supervise the reasoning process itself? Differences
                   between these approaches might naturally be expected not just
                   in final-answer errors but also in reasoning errors, which
                   can be difficult to detect and are problematic in many
                   real-world domains such as education. We run the first
                   comprehensive comparison between process- and outcome-based
                   approaches trained on a natural language task, GSM8K. We find
                   that pure outcome-based supervision produces similar
                   final-answer error rates with less label supervision.
                   However, for correct reasoning steps we find it necessary to
                   use process-based supervision or supervision from learned
                   reward models that emulate process-based feedback. In total,
                   we improve the previous best results from 16.8\% $\to$ 12.7\%
                   final-answer error and 14.0\% $\to$ 3.4\% reasoning error
                   among final-answer-correct solutions.",
  month         =  "25~" # nov,
  year          =  2022,
  url           = "http://arxiv.org/abs/2211.14275",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  keywords      = "o1"
}

@ARTICLE{Lightman2023-cr,
  title         = "Let's verify step by step",
  author        = "Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and
                   Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan
                   and Schulman, John and Sutskever, Ilya and Cobbe, Karl",
  journal       = "arXiv [cs.LG]",
  abstract      = "In recent years, large language models have greatly improved
                   in their ability to perform complex multi-step reasoning.
                   However, even state-of-the-art models still regularly produce
                   logical mistakes. To train more reliable models, we can turn
                   either to outcome supervision, which provides feedback for a
                   final result, or process supervision, which provides feedback
                   for each intermediate reasoning step. Given the importance of
                   training reliable models, and given the high cost of human
                   feedback, it is important to carefully compare the both
                   methods. Recent work has already begun this comparison, but
                   many questions still remain. We conduct our own
                   investigation, finding that process supervision significantly
                   outperforms outcome supervision for training models to solve
                   problems from the challenging MATH dataset. Our
                   process-supervised model solves 78\% of problems from a
                   representative subset of the MATH test set. Additionally, we
                   show that active learning significantly improves the efficacy
                   of process supervision. To support related research, we also
                   release PRM800K, the complete dataset of 800,000 step-level
                   human feedback labels used to train our best reward model.",
  month         =  "31~" # may,
  year          =  2023,
  url           = "http://arxiv.org/abs/2305.20050",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  keywords      = "o1"
}

@ARTICLE{Wang2022-px,
  title         = "Self-Consistency Improves Chain of Thought Reasoning in
                   Language Models",
  author        = "Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc
                   and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and
                   Zhou, Denny",
  journal       = "arXiv [cs.CL]",
  abstract      = "Chain-of-thought prompting combined with pre-trained large
                   language models has achieved encouraging results on complex
                   reasoning tasks. In this paper, we propose a new decoding
                   strategy, self-consistency, to replace the naive greedy
                   decoding used in chain-of-thought prompting. It first samples
                   a diverse set of reasoning paths instead of only taking the
                   greedy one, and then selects the most consistent answer by
                   marginalizing out the sampled reasoning paths.
                   Self-consistency leverages the intuition that a complex
                   reasoning problem typically admits multiple different ways of
                   thinking leading to its unique correct answer. Our extensive
                   empirical evaluation shows that self-consistency boosts the
                   performance of chain-of-thought prompting with a striking
                   margin on a range of popular arithmetic and commonsense
                   reasoning benchmarks, including GSM8K (+17.9\%), SVAMP
                   (+11.0\%), AQuA (+12.2\%), StrategyQA (+6.4\%) and
                   ARC-challenge (+3.9\%).",
  month         =  "21~" # mar,
  year          =  2022,
  url           = "http://arxiv.org/abs/2203.11171",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  keywords      = "MiniChain;o1"
}
